%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% LaTeX book template                           %%
%% Author:  Amber Jain (http://amberj.devio.us/) %%
%% License: ISC license                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[a4paper,11pt]{article}
\bibliographystyle{plain}
\usepackage[margin=1.2in]{geometry}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{paracol}
\usepackage{lmodern}
\usepackage{amsfonts}
\usepackage{multicol}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage[english]{babel}
\usepackage{microtype} 
\usepackage[justification=centering]{caption}  % centers captions automatically
%\usepackage{caption2}
\usepackage{subcaption}

\usepackage{titlesec}
\renewcommand{\thesection}{}% Remove section references...
\renewcommand{\thesubsection}{\arabic{subsection}}%... from subsections

\usepackage[version=3]{mhchem}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage[absolute,overlay]{textpos}
\usepackage{graphicx}
\usepackage{bm}
\usepackage[normalem]{ulem}
\usepackage{cancel}
\usepackage{relsize}
\usepackage{enumitem}
\usepackage{pgfpages}
\usepackage{floatrow}  % centers images automatically
\usepackage[longnamesfirst, authoryear]{natbib} 

\bibliographystyle{plainnat}

\definecolor{ForestGreen}{rgb}{0.13, 0.55, 0.13}


\newcommand\norm[1]{\left\lVert#1\right\rVert}
\newcommand\red[1]{\textcolor{red}{\textbf{#1}}}
\newcommand\blue[1]{\textcolor{blue}{\textbf{#1}}}
\newcommand\yellow[1]{\textcolor{yellow}{\textbf{#1}}}
\newcommand\green[1]{\textcolor{ForestGreen}{\textbf{#1}}}
\newcommand\ds[1]{\displaystyle{#1}}
\renewcommand\b[1]{\textbf{#1}}
\DeclarePairedDelimiterX{\ip}[2]{\langle}{\rangle}{#1, #2}
\DeclareMathOperator*{\argmax}{arg\,max}


\setcounter{tocdepth}{3}
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
\setlength{\parindent}{0pt}
\setlength{\parskip}{2ex plus 0.5ex minus 0.2ex}

\captionsetup[figure]{font=small,labelfont=small}
\setlength{\belowcaptionskip}{-5pt}
\setlist[itemize]{noitemsep, topsep=2pt}
\setlist[enumerate]{noitemsep, topsep=2pt}
\setlength\parindent{0pt}



% Author
\author{\textsc{Y. Fan, M. Andriushchenko}}


\begin{document}
	\begin{titlepage}
		\newgeometry{margin=3cm}
		\centering
		\vspace*{\stretch{0.5}}
		\Large \textbf{Joint Training of a Convolutional Network and a Graphical Model for Human Pose Estimation}
		\vspace{\stretch{0.5}}
		\normalsize Maksym Andriushchenko \\
		\normalsize 2565540 \\
		\normalsize \& \\
		\normalsize FAN Yue \\
		\normalsize 2564216 \\
		\vspace{\stretch{0.5}}
		\normalsize Saarland Univ.
	\end{titlepage}

\newpage
	\section{Abstract}
%	\red{\textbf{Small warning}: We did not use the Virtual Machine and installed all the packages from scratch! We had to modify 2 lines of code:
%		\begin{enumerate}
%			\item np.zeros((360*k+1, 2)) -> np.zeros((int(360*k)+1, 2))
%			\item axarr[0].get\_axes().set\_aspect(1) -> axarr[0].axes.set\_aspect(1).
%		\end{enumerate}
%	}
	We address the problem of single person pose estimation from 2D images.
	Our method is aspired from the paper Joint Training of a Convolutional Network and a Graphical Model for Human Pose
	Estimation, the model is a combination of a CNN and a PGM trained end-to-end jointly. We reproduce the same results
	of the authors and explore that with Batch Normalization, we achieve a faster convergence.

\newpage
	\section{1. Problem Overview}

\newpage
	\section{2. Methods}
\newpage
	\section{3. Evaluation}
\newpage
	\section{4. Future work}


    Structure:
    - Problem description
    - Your approach
    - Results / Interpretation

    Idea: CNNs are great, but can't be easily controlled. We need to impose strong prior knowledge on the
    relations between parts.


    To mention in the final report:
    - we do it much faster using BN
    - we use an auxiliary classifier (Inception style or like in Fast R-CNN: "multi-task loss") to make both PD and SM perform well
    This goes in line with motivation from the paper.
    - thus we can train the model end-to-end from scratch and much faster (30 minutes instead of 60 hours)
    but we should train on FLIC+...
    - we provide understanding on what the model learns
    - we fix the mistakes from the paper and fill the gaps (leaves mixed feeling, how could it be that they mixed up
    the dimensions of the convolutions; if you really train model X, you just translate it to the description)
    - we show how can we improve the pairwise potentials if we optimize them.

    The paper leaves mixed feeling, especially because they didn't explain what their SM learn.
    On Fig. 5 they showed "a didactic example", which they most probably drew by hand.
    It would be very interesting to see the pairwise heatmaps obtained after backprop. We show them: ...
    Thus, our contribution is not only in practical implementation of the paper, but also in understanding what the proposed
    model actually learns.

    We claim that their statement "The learned pair-wise distributions are purely uniform when any pairwise edge should to
    be removed from the graph structure" is wrong. Even connections that don't appear in a traditional star model are not
    uniform. E.g. relation between lhip and rwri. Of course, their relative positions can vary a lot, but there are certainly
    regions that have 0 probability (e.g. too far away): *show picture*

    Smart init makes sense: test_dr 0.2% 7.1% with random weights


    WD only over conv filters, since we observed that the highest values of pairwise potentials and biases are
    quite moderate, so it doesn't make sense to include WD there.

    Another contribution: we show how to perform joint training immediately with a single set of hps.

    softplus -> relu? no motivation why softplus is used

    Discuss the magnitude of grads wrt pairwise energies/biases (should be small => training is successful).

    Discuss that conv filters are like edge/color detectors.

    Cross entropy: much faster convergence.

    Eldar et. al (DeepCut): Hyperparameter search is very important (especially if the hps were not reported!).

    ResNets paper: use downsampling with the stride=2 in the beginning (unlike in the paper, we don't lose the information!).

    More advanced DA.

    SM takes half of the time needed for PD.

    Show evolution of pairwise potentials over iters (select from those that are arleady lwri|lelb. however, the role of
    pairwise pots is slightly less useful for LSP)


    We excluded self connections like face|face

    show a principled plot of test MSE between PD and SM.

    Boring but important implementation detail: BN is not so efficient in multi-gpu training.

    With Momentum the spatial model can't be properly trained. Thus adaptive learning rates (Adam).

    Interesting Q: how detection rate correlates with MSE?



    We tried, but the improvement after SM is only up to 6% for r=10.
    But anyway, it gives a signficiant and consistent improvement.

    We can't confirm the benefit of joint training. It makes no further benefit compared to
    separate training of PD and SM

    However, we improved the PD.

    tell about it clearly in Github => any suggestions are welcome, but the paper didn't make a good
    job in covering details, so it's (again!!!) not reproducible

    note, that using e.g. Resnets as the PD can improve the performance considerably (cite deeper cut)
    this can be explained by the high receptive field size, which means that the PD can
    learn useful spatial relations between parts (cite conv. pose machines https://arxiv.org/pdf/1602.00134.pdf)
    <provide their plot>
    it's easier to recognize right wrists than left wrists

    (disappointment) Actually our paper is just a subnetwork which tries to capture the kinematic constraints.
    Simply using a deeper PD can perform better than the approach proposed in the paper...
    But it was a good way to explore how powerful can be a PGM on top of CNN.

    Another problems:
    - seems that they cropped a bit of width (but didn't clear how arrived to 90x60 heatmaps)
    - ignored downsampling x2 and why they did it
    - they show plot for left wrist, because it's better than the right wrist, but don't mention this!!!
    - lsho rhip and not rhip lsho because it gives slightly better result (again, they don't mention this)


    Hilarious comment of the authors:
    % Note that this makes backwards-facing people have "incorrect" annotation
    % according to human anatomy, but oh-so-right when it comes to training and
    % testing front/back-agnostic 2D human layout models.
    This is not something reported in the papers. However, this +3-4% clearly make a difference between SOTA or not.
    (4.72% backward poses in the test set, 4.99% in the train set)


    Good sign: if we calculate cross-correlation instead of convolution we get much less imporvement by SM (1-2%).
\newpage
	\begin{thebibliography}{}
		\bibitem{cnn_pgm_for_hpe}
		\href{https://arxiv.org/abs/1406.2984}
		{Joint Training of a Convolutional Network and a Graphical Model for Human Pose Estimation}

		\bibitem{cnn_pgm_for_hpe}
		\href{https://arxiv.org/abs/1312.7302}
		{Learning Human Pose Estimation Features with Convolutional Networks}

		\bibitem{cnn_pgm_for_hpe}
		\href{https://homes.cs.washington.edu/~taskar/pubs/modec_cvpr13.pdf}
		{Multimodal decomposable models for human pose estimation}
	\end{thebibliography}
\newpage

\end{document}


